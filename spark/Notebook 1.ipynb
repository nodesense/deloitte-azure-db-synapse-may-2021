{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkCluster",
              "session_id": 0,
              "statement_id": 1,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-05-27T08:03:24.5419913Z",
              "execution_start_time": "2021-05-27T08:06:12.7543876Z",
              "execution_finish_time": "2021-05-27T08:06:16.9212126Z"
            },
            "text/plain": "StatementMeta(SparkCluster, 0, 1, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "res6: Class[_] = class com.microsoft.sqlserver.jdbc.SQLServerDriver\nhostname: String = deloittesynapase-ondemand.sql.azuresynapse.net\nport: Int = 1433\ndatabase: String = moviedb\njdbcUrl: String = jdbc:sqlserver://deloittesynapase-ondemand.sql.azuresynapse.net:1433;database=moviedb\nimport java.util.Properties\nprops: java.util.Properties = {}\nres11: Object = null\nres12: Object = null\ndriverClass: String = com.microsoft.sqlserver.jdbc.SQLServerDriver\nres15: Object = null\n"
        }
      ],
      "metadata": {},
      "source": [
        "// https://techcommunity.microsoft.com/t5/azure-synapse-analytics/query-serverless-sql-pool-from-an-apache-spark-scala-notebook/ba-p/2250968\r\n",
        "\r\n",
        "// Define connection:\r\n",
        "Class.forName(\"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\r\n",
        "\r\n",
        "val hostname = \"deloittesynapase-ondemand.sql.azuresynapse.net\"\r\n",
        "val port = 1433\r\n",
        "val database = \"moviedb\" // If needed, change the database \r\n",
        "val jdbcUrl = s\"jdbc:sqlserver://${hostname}:${port};database=${database}\"\r\n",
        "\r\n",
        "// Define connection properties:\r\n",
        "import java.util.Properties\r\n",
        "\r\n",
        "val props = new Properties()\r\n",
        "props.put(\"user\", \"sqladminuser\")\r\n",
        "props.put(\"password\", \"AzSyG2May2021\")\r\n",
        "\r\n",
        "// Assign driver to connection:\r\n",
        "val driverClass = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\r\n",
        "props.setProperty(\"Driver\", driverClass)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkCluster",
              "session_id": 0,
              "statement_id": 7,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-05-27T08:17:00.4039673Z",
              "execution_start_time": "2021-05-27T08:17:00.5311525Z",
              "execution_finish_time": "2021-05-27T08:17:04.6530761Z"
            },
            "text/plain": "StatementMeta(SparkCluster, 0, 7, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "q: String =\n\"\nselect 1 + 2 as r\n\"\nobjects: org.apache.spark.sql.DataFrame = [r: int]\n+---+\n|  r|\n+---+\n|  3|\n+---+\n\n"
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "val q = \"\"\"\r\n",
        "select 1 + 2 as r\r\n",
        "\"\"\"\r\n",
        "val objects = spark.read.jdbc(jdbcUrl, s\"(${q}) res\", props)\r\n",
        "objects.show(10)\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkCluster",
              "session_id": 0,
              "statement_id": 6,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-05-27T08:15:30.2673511Z",
              "execution_start_time": "2021-05-27T08:15:30.3814232Z",
              "execution_finish_time": "2021-05-27T08:15:32.455446Z"
            },
            "text/plain": "StatementMeta(SparkCluster, 0, 6, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "Error",
          "evalue": "com.microsoft.sqlserver.jdbc.SQLServerException: Incorrect syntax near the keyword 'select'.",
          "traceback": [
            "Error: com.microsoft.sqlserver.jdbc.SQLServerException: Incorrect syntax near the keyword 'select'.",
            "  at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:262)\n",
            "  at com.microsoft.sqlserver.jdbc.SQLServerStatement.getNextResult(SQLServerStatement.java:1632)\n",
            "  at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.doExecutePreparedStatement(SQLServerPreparedStatement.java:602)\n",
            "  at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement$PrepStmtExecCmd.doExecute(SQLServerPreparedStatement.java:524)\n",
            "  at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7375)\n",
            "  at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3206)\n",
            "  at com.microsoft.sqlserver.jdbc.SQLServerStatement.executeCommand(SQLServerStatement.java:247)\n",
            "  at com.microsoft.sqlserver.jdbc.SQLServerStatement.executeStatement(SQLServerStatement.java:222)\n",
            "  at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeQuery(SQLServerPreparedStatement.java:446)\n",
            "  at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:61)\n",
            "  at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:210)\n",
            "  at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:35)\n",
            "  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:317)\n",
            "  at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n",
            "  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:212)\n",
            "  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:168)\n",
            "  at org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:243)\n",
            "  ... 52 elided\n"
          ]
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "val objects = spark.read.jdbc(jdbcUrl, \"select 1 + 2 as t\", props)\r\n",
        "objects.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_spark",
      "display_name": "Synapse Spark"
    },
    "language_info": {
      "name": "scala"
    },
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  }
}